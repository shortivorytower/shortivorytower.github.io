<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>About Principal Components Analysis - Short Ivory Tower</title>

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P3RGW6V');</script>
<!-- End Google Tag Manager -->


  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Short Ivory Tower" property="og:site_name">
  
    <meta content="About Principal Components Analysis" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="This blog is a place for me to write down technical stuff that I found interesting. 
" property="og:description">
  
  
    <meta content="/about-principal-components-analysis/" property="og:url">
  
  
    <meta content="2020-09-13T00:00:00+08:00" property="article:published_time">
    <meta content="/about/" property="article:author">
  
  
    <meta content="/assets/img/hk-add-oil.jpg" property="og:image">
  
  
    
  
  
    
    <meta content="Statistics" property="article:tag">
    
    <meta content="Mathematics" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@">
  
    <meta name="twitter:title" content="About Principal Components Analysis">
  
  
    <meta name="twitter:url" content="/about-principal-components-analysis/">
  
  
    <meta name="twitter:description" content="This blog is a place for me to write down technical stuff that I found interesting. 
">
  
  
    <meta name="twitter:image:src" content="/assets/img/hk-add-oil.jpg">
  

	<meta name="description" content="">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P3RGW6V"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/hk-add-oil.jpg" alt="Short Ivory Tower"></a>
      </div>
      <div class="author-name">Short Ivory Tower</div>
      <p>This blog is a notepad to write down technical stuff that I found interesting.</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
	<!--
          <li><a href="https://twitter.com/artemsheludko_" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
	-->
        
        
	<!--
          <li><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
	-->
        
        
          <li class="github"><a href="http://github.com/shortivorytower" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
	<!--
          <li class="linkedin"><a href="https://in.linkedin.com/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a></li>
	-->
        
        
	<!--
          <li class="email"><a href="mailto:example.david@blog.com"><i class="fa fa-envelope-o" aria-hidden="true"></i></a></li>
	-->
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2020 &copy; Short Ivory Tower</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">About Principal Components Analysis</h1>
        <div class="page-date"><span>2020, Sep 13&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
                  tex2jax: {
                    inlineMath: [['$','$'], ['\\(','\\)']], 
                    displayMath: [ ['$$','$$'], ['\[','\]'] ],
                    processEnvironments: true
                  }
                });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!--
  >
-->

<p>In layman terms, Principal Components Analysis is a technique to systematically break down some random variables fluctuation and relations, and to simplify our task of analysing multiple random data sources. It has many useful applications such as dimension reduction, visualization of higher dimensional data, image compression, as well as statistical factor modelling.</p>

<p>More mathematically, it decomposes the covariance matrix of a set of <script type="math/tex">n</script> zero-centered random variables using a technique called eigendecomposition, and that gives a set of <script type="math/tex">n</script> column vectors <script type="math/tex">u_1, u_2, \cdots u_n</script> called eigenvectors, and a diagonal matrix that consists of <script type="math/tex">n</script> scalar <script type="math/tex">\lambda_1, \lambda_2, \cdots \lambda_n</script> called eigenvalues. The eigenvectors are the <em>principal components</em>, in which they are orthogonal (i.e. perpendicular to each other), and pointing at the directions that best explains the variance of the random data.</p>

<p>Suppose we have put <script type="math/tex">n</script> random variables <script type="math/tex">X_1, X_2, \cdots X_n</script>, with <script type="math/tex">p</script> observations in a <script type="math/tex">p \times n</script> matrix <script type="math/tex">\boldsymbol{X}</script>, such that the <script type="math/tex">i</script>th column represents the observations of <script type="math/tex">X_i</script>.</p>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{X} =
\begin{pmatrix}
x_{1,1} & x_{1,2} & \cdots & x_{1,n} \\
x_{2,1} & x_{2,2} & \vdots & x_{2,n} \\
\vdots & \vdots & \vdots & \vdots  \\
x_{p-1,1} & x_{p-1,2} & \vdots & x_{p-1,n} \\
x_{p,1} & x_{p,2} & \cdots & x_{p,n} \\
\end{pmatrix} %]]></script>

<p>To perform principal components analysis, we first mean centered all <script type="math/tex">n</script> random variables <script type="math/tex">X_1, X_2, \cdots X_n</script> observations.</p>

<p><script type="math/tex">Y_i = X_i - \overline{X}_i</script> for <script type="math/tex">i = 1,2,\cdots n</script></p>

<p>Then we compute the covariance matrix <script type="math/tex">C</script> for all those  <script type="math/tex">Y_i</script> observations.</p>

<script type="math/tex; mode=display">% <![CDATA[
C = 
\begin{pmatrix}
Cov(Y_1, Y_1) & Cov(Y_1, Y_2) & \cdots & \cdots \\
Cov(Y_2, Y_1) & Cov(Y_2, Y_2) & \cdots & \cdots \\
\vdots & \vdots & \ddots & Cov(Y_{n-1}, Y_n) \\
\cdots & \cdots & Cov(Y_n, Y_{n-1}) & Cov(Y_n, Y_n) \\
\end{pmatrix} %]]></script>

<p>Then we compute the eigendecomposition on <script type="math/tex">C</script>, such that <script type="math/tex">C = U \Lambda U^{-1}</script>.</p>

<p>Since <script type="math/tex">C</script> is a real  $ n \times n $ symmetric matrix,</p>

<script type="math/tex; mode=display">C = U \Lambda U^T</script>

<p>where each column of <script type="math/tex">U</script> is the eigenvector of  <script type="math/tex">C</script>. Denoting <script type="math/tex">\boldsymbol{u_i}</script> as the <script type="math/tex">i</script>th column vector of <script type="math/tex">U</script> such that</p>

<script type="math/tex; mode=display">\boldsymbol{u_i} = \begin{pmatrix}
u_{1,i} \\
u_{2,i} \\
\vdots \\
u_{n,i}
\end{pmatrix}</script>

<p>and</p>

<script type="math/tex; mode=display">% <![CDATA[
U = \begin{pmatrix}
u_{1,1} & u_{1,2} & u_{1,3} & \cdots & u_{1,n} \\
u_{2,1} & u_{2,2} & u_{2,3} & \cdots & u_{2,n} \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
\vdots & \vdots & \ddots & \ddots & u_{n-1,n} \\
u_{n,1} & u_{n,2} & u_{n,3} & \cdots & u_{n,n} \\
\end{pmatrix} %]]></script>

<p>and the diagonal values of <script type="math/tex">\Lambda</script> are the eigenavlues of <script type="math/tex">C</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\Lambda = \begin{pmatrix}
\lambda_1 & 0 & \cdots & \cdots & 0 \\
0 & \lambda_2  & \cdots& \cdots & 0 \\
\vdots & \vdots & \ddots & \ddots & 0 \\
\vdots & \vdots & \cdots & \lambda_{n-1} & 0 \\
0 & 0  & \cdots & \cdots & \lambda_n \\
\end{pmatrix} %]]></script>

<p>Note the result column vectors (i.e. eigenvector) <script type="math/tex">\boldsymbol{u_i}</script> are the <em>principal components</em>. They are are orthogonal. Therefore their dot products between each other are zero and with themselves are one.</p>

<script type="math/tex; mode=display">U^T U = I</script>

<p><script type="math/tex">\boldsymbol{u_1}</script> is the first principal component and <script type="math/tex">\boldsymbol{u_2}</script> is the second princiapl component and so on. These principal components are best geometrically positioned to explain the covariance of the original observations in the matrix <script type="math/tex">\boldsymbol{X}</script>, weighted by the value of <script type="math/tex">\lambda_i</script>.</p>

<p>The explained variance of <script type="math/tex">i</script>th principal component is given by <script type="math/tex">\frac{\lambda_i}{\sum_{j=1}^{n} \lambda_j}</script>.</p>

<p>Once we have the principal components, each data point in the matrix <script type="math/tex">\boldsymbol{Y}</script> (i.e. mean centered version of <script type="math/tex">\boldsymbol{X}</script> ) can be projected to the principal components and can be reconstructed back to the original data (if we take every principal components).</p>

<p>To do this, first we project the data point in <script type="math/tex">\boldsymbol{Y}</script> into the principal components matrix <script type="math/tex">U</script> to get a scores matrix <script type="math/tex">W</script>.</p>

<script type="math/tex; mode=display">W = \boldsymbol{Y} U</script>

<p>Then each row of <script type="math/tex">W</script> is the scores of the data point on each principal components. 
For example <script type="math/tex">w_{1,1}, w_{1,2}, w_{1,3}</script> are the scores of the first data point (i.e. the first row of <script type="math/tex">\boldsymbol{Y}</script>) for the principal component <script type="math/tex">\boldsymbol{u_1}, \boldsymbol{u_2}, \boldsymbol{u_3}</script> respectively.</p>

<p>To reconstruct the original data using the scores <script type="math/tex">W</script> and principal components <script type="math/tex">U</script>, we can simply do this multiplication,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\boldsymbol{Y_{new}} &= W U^T  \\ 

\boldsymbol{X_{new}} &= \boldsymbol{Y_{new}} + \overline{\boldsymbol{X}} \\

\end{align*} %]]></script>

<p>We can also take less number of principal components to reconstruct the data sacrificing some details.</p>

<p>Implementation with some random data:</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/0a5cc05c5f477a2926fe8e0209106e23.js"> </script>

<p>Output:</p>

<figure class="highlight"><pre><code class="language-txt" data-lang="txt">Covariance can be reconstructed: True
Explained Variance: [0.43797883 0.10940073 0.25062846 0.20199197]
Reconstructed data matched: True
Result Matrix difference if we take only 3 PCs
[[-0.00808952  0.0325239   0.02538962  0.01339734]
 [ 0.00878714 -0.03532867 -0.02757915 -0.01455269]
 [-0.01415474  0.05690906  0.04442577  0.02344214]
 [ 0.00114743 -0.00461325 -0.00360131 -0.0019003 ]
 [-0.00050191  0.00201791  0.00157527  0.00083122]
 [-0.01042606  0.04191792  0.03272301  0.01726695]
 [-0.00962128  0.03868232  0.03019716  0.01593413]
 [ 0.00485241 -0.0195091  -0.01522968 -0.00803624]
 [ 0.01948955 -0.07835768 -0.06116953 -0.03227732]
 [ 0.00851696 -0.03424241 -0.02673116 -0.01410523]]</code></pre></figure>


      <div class="page-footer">
        <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=About Principal Components Analysis&url=/about-principal-components-analysis/" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=/about-principal-components-analysis/" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=/about-principal-components-analysis/" title="Share on Google+" rel="nofollow" target="_blank">Google+</a>
        </div>
        <div class="page-tag">
          
            <a href="/tags#Statistics" class="tag">&#35; Statistics</a>
          
            <a href="/tags#Mathematics" class="tag">&#35; Mathematics</a>
          
        </div>
      </div>
      <section class="comment-area">
  <div class="comment-wrapper">
    
    <div id="disqus_thread" class="article-comments"></div>
    <script>
      (function() {
          var d = document, s = d.createElement('script');
          s.src = '//shortivorytower.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
  </div>
</section> <!-- End Comment Area -->

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>
  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
</html>
