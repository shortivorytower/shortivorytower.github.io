<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Some Notes on Matrix Calculus - Short Ivory Tower</title>

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-P3RGW6V');</script>
<!-- End Google Tag Manager -->


  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Short Ivory Tower" property="og:site_name">
  
    <meta content="Some Notes on Matrix Calculus" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="This blog is a place for me to write down technical stuff that I found interesting. 
" property="og:description">
  
  
    <meta content="/matrix-calculus/" property="og:url">
  
  
    <meta content="2017-07-04T00:00:00+08:00" property="article:published_time">
    <meta content="/about/" property="article:author">
  
  
    <meta content="/assets/img/hk-add-oil.jpg" property="og:image">
  
  
    
    <meta content="math" property="article:section">
    
  
  
    
    <meta content="Mathematics" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@">
  
    <meta name="twitter:title" content="Some Notes on Matrix Calculus">
  
  
    <meta name="twitter:url" content="/matrix-calculus/">
  
  
    <meta name="twitter:description" content="This blog is a place for me to write down technical stuff that I found interesting. 
">
  
  
    <meta name="twitter:image:src" content="/assets/img/hk-add-oil.jpg">
  

	<meta name="description" content="">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P3RGW6V"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/hk-add-oil.jpg" alt="Short Ivory Tower"></a>
      </div>
      <div class="author-name">Short Ivory Tower</div>
      <p>This blog is a notepad to write down technical stuff that I found interesting.</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
	<!--
          <li><a href="https://twitter.com/artemsheludko_" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
	-->
        
        
	<!--
          <li><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
	-->
        
        
          <li class="github"><a href="http://github.com/shortivorytower" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
	<!--
          <li class="linkedin"><a href="https://in.linkedin.com/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a></li>
	-->
        
        
	<!--
          <li class="email"><a href="mailto:example.david@blog.com"><i class="fa fa-envelope-o" aria-hidden="true"></i></a></li>
	-->
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2019 &copy; Short Ivory Tower</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Some Notes on Matrix Calculus</h1>
        <div class="page-date"><span>2017, Jul 04&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
                  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                          });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>Recently I registered Andrew Ng’s Machine learning in Coursera. Now it is at week 2 and in fact the content is still quite trivial.
At the moment he is teaching a simplified version of multiple linear regression (e.g. Andrew didn’t mention how to eliminate the parameters using T tests etc).
I think his intention here is only to demonstrate the supervised learning framework and ends up further involved later in the course where we would learn more on Neural Network and Stochastic Gradient Descent.</p>

<p>When he taught the normal equation of multiple linear regression, perhaps he was worried about the mathematical background of the online students. He just stated the answer:</p>

<p>Given sample input <script type="math/tex">X</script>  and sample output <script type="math/tex">y</script>，the linear regression optimal parameter vector <script type="math/tex">\theta</script> is:</p>

<script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^Ty</script>

<p>Assume we have <script type="math/tex">n</script> parameters and <script type="math/tex">m</script> training samples, here <script type="math/tex">\theta</script> is a column vector of size <script type="math/tex">n</script>, <script type="math/tex">y</script> is a column vector of size <script type="math/tex">m</script>, and <script type="math/tex">X</script> is a matrix of size <script type="math/tex">m \times n</script>.</p>

<p>Actually for the purpose of this course we don’t need to know how to get this result. We can just take it as granted, but I just want to try deriving it myself.</p>

<p>The cost function of Linear Regression is the sum of squared training error (residuals). Or in matrix form that is, 
<script type="math/tex">J(\theta) = (y - X \theta)^T(y-X\theta)</script></p>

<p>We want to solve for a <script type="math/tex">\theta</script> such that <script type="math/tex">\frac{\partial J(\theta)}{\partial \theta} =  0</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
J(\theta) & = (y - X \theta)^T(y-X\theta) \\ 
& = (y^T - \theta^TX^T)(y-X\theta) \\
& = (y^Ty - \theta^TX^Ty - y^TX\theta + \theta^TX^TX\theta) \\
\frac{\partial J(\theta)}{\partial \theta} & = 
\frac{\partial y^Ty}{\partial \theta}
-\frac{\partial \theta^TX^Ty}{\partial \theta} 
-\frac{\partial y^TX\theta}{\partial \theta} 
+\frac{\partial \theta^TX^TX\theta}{\partial \theta} \\
& = -\frac{\partial \theta^TX^Ty}{\partial \theta} 
-\frac{\partial y^TX\theta}{\partial \theta} 
+\frac{\partial \theta^TX^TX\theta}{\partial \theta} \\
& = 0 \\
\end{align*} %]]></script>

<p>Up to this point in fact I don’t know how to derive further because I don’t know how to do differentiation with matrices and vectors.
After lots of google searching eventually I’ve found some introduction documents and learned some well known formulae that help.</p>

<p>NOTE: There are two Layout Convention in Matrix Calculus. All formulas below are in <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Denominator-layout_notation"><strong>Denominator layout</strong></a>. i.e. <script type="math/tex">\frac{\partial y}{\partial x}</script> We always take transpose on <script type="math/tex">y</script> and preserve the <script type="math/tex">x</script> dimension. If we use <strong>Numerator Layout</strong> all formulae below have to be changed.</p>

<p>Referring to <a href="http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf">The Matrix Cookbook</a>, if <script type="math/tex">\textbf{a}</script> and <script type="math/tex">\textbf{x}</script> are column vector, and <script type="math/tex">A</script> is a Matrix，</p>

<script type="math/tex; mode=display">\begin{equation}  
\frac{\partial \textbf{x}^T\textbf{a}}{\partial \textbf{x}} = \textbf{a}  
\end{equation}</script>

<script type="math/tex; mode=display">\begin{equation}  
\frac{\partial \textbf{a}^T\textbf{x}}{\partial \textbf{x}} = \textbf{a}  
\end{equation}</script>

<script type="math/tex; mode=display">\begin{equation}  
\frac{\partial \textbf{x}^TA\textbf{x}}{\partial \textbf{x}} = (A+A^T)\textbf{x}  
\end{equation}</script>

<p>Using the identities above,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\frac{\partial \theta^TX^Ty}{\partial \theta} & = X^Ty \\
\frac{\partial y^TX\theta}{\partial \theta} & = (y^TX)^T = X^Ty \\
\frac{\partial \theta^TX^TX\theta}{\partial \theta} & = \Big[X^TX+(X^TX)^T\Big]\theta = 2X^TX\theta\\
\end{align*} %]]></script>

<p>Substitute these to <script type="math/tex">\frac{\partial J(\theta)}{\partial \theta}</script>,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\frac{\partial J(\theta)}{\partial \theta} = -X^Ty -X^Ty + 2X^TX\theta & = 0 \\
2X^TX\theta & = 2X^Ty \\
\theta & = (X^TX)^{-1}X^Ty
\end{align*} %]]></script>

<p>When I try to understand matrix differentiation, I also picked up the meaning of partial differentiation with respect to matrix/vector. In fact there are only a limited number of scalars/vector/matrix combinations.</p>

<p>If <script type="math/tex">f\colon \mathbb{R}^n\to\mathbb{R}</script> and <script type="math/tex">\mathbb{x}\in\mathbb{R}^n</script>，<script type="math/tex">% <![CDATA[
\mathbb{x} = \begin{pmatrix} x_1 & x_2 & \cdots & x_n \end{pmatrix}^T %]]></script>, we get the Gradient:</p>

<script type="math/tex; mode=display">\frac{\partial f(\mathbb{x})}{\partial \mathbb{x}} = 
\begin{pmatrix}
\frac{\partial f(\mathbb{x})}{\partial x_1} \\
\frac{\partial f(\mathbb{x})}{\partial x_2} \\
\vdots \\
\frac{\partial f(\mathbb{x})}{\partial x_n} 
\end{pmatrix}</script>

<p>Second Order differentiated we get Hessian Matrix:</p>

<script type="math/tex; mode=display">% <![CDATA[
\frac{\partial^2 f(\mathbb{x})}{\partial \mathbb{x}^2} = 
\begin{pmatrix}

\frac{\partial^2 f(\mathbb{x})}{\partial x_1^2} & \frac{\partial^2 f(\mathbb{x})}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f(\mathbb{x})}{\partial x_1 \partial x_n} \\

\frac{\partial^2 f(\mathbb{x})}{\partial x_2 \partial x_1} & \frac{\partial^2 f(\mathbb{x})}{\partial x_2^2} & \cdots & \frac{\partial^2 f(\mathbb{x})}{\partial x_2 \partial x_n} \\

\vdots & \vdots & \ddots & \vdots \\

\frac{\partial^2 f(\mathbb{x})}{\partial x_n \partial x_1} & \frac{\partial^2 f(\mathbb{x})}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f(\mathbb{x})}{\partial x_n^2} 

\end{pmatrix} %]]></script>

<p>If <script type="math/tex">\mathbb{f}\colon \mathbb{R}^n\to\mathbb{R}^m</script> ，
<script type="math/tex">% <![CDATA[
\mathbb{f}(\mathbb{x}) = \begin{pmatrix}f_1(\mathbb{x}) & f_2(\mathbb{x}) & \cdots & f_m(\mathbb{x}) \end{pmatrix}^T %]]></script></p>

<p>and <script type="math/tex">\mathbb{x}\in\mathbb{R}^n</script>, we get Jacobian Matrix:</p>

<script type="math/tex; mode=display">% <![CDATA[
\frac{\partial \mathbb{f}(\mathbb{x})}{\partial \mathbb{x}} = 
\begin{pmatrix}

\frac{\partial f_1(\mathbb{x})}{\partial x_1} & \frac{\partial f_2(\mathbb{x})}{\partial x_1} & \cdots & \frac{\partial f_m(\mathbb{x})}{\partial x_1} \\

\frac{\partial f_1(\mathbb{x})}{\partial x_2} & \frac{\partial f_2(\mathbb{x})}{\partial x_2} & \cdots & \frac{\partial f_m(\mathbb{x})}{\partial x_2} \\

\vdots & \vdots & \ddots & \vdots \\

\frac{\partial f_1(\mathbb{x})}{\partial x_n} & \frac{\partial f_2(\mathbb{x})}{\partial x_n} & \cdots & \frac{\partial f_m(\mathbb{x})}{\partial x_n} 

\end{pmatrix} %]]></script>

<p>If <script type="math/tex">f\colon \mathbb{R}^{m \times n}\to\mathbb{R}</script>，and <script type="math/tex">\textbf{A}\in\mathbb{R}^{m \times n}</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\textbf{A} = 
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} 
\end{pmatrix} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\frac{\partial f(\textbf{A})}{\partial \textbf{A}} = 
\begin{pmatrix}

\frac{\partial f(\textbf{A})}{\partial a_{11}} & \frac{\partial f(\textbf{A})}{\partial a_{12}} & \cdots & \frac{\partial f(\textbf{A})}{\partial a_{1n}} \\

\frac{\partial f(\textbf{A})}{\partial a_{21}} & \frac{\partial f(\textbf{A})}{\partial a_{22}} & \cdots & \frac{\partial f(\textbf{A})}{\partial a_{2n}} \\

\vdots & \vdots & \ddots & \vdots \\

\frac{\partial f(\textbf{A})}{\partial a_{m1}} & \frac{\partial f(\textbf{A})}{\partial a_{m2}} & \cdots & \frac{\partial f(\textbf{A})}{\partial a_{mn}} 

\end{pmatrix} %]]></script>


      <div class="page-footer">
        <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=Some Notes on Matrix Calculus&url=/matrix-calculus/" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=/matrix-calculus/" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=/matrix-calculus/" title="Share on Google+" rel="nofollow" target="_blank">Google+</a>
        </div>
        <div class="page-tag">
          
            <a href="/tags#Mathematics" class="tag">&#35; Mathematics</a>
          
        </div>
      </div>
      <section class="comment-area">
  <div class="comment-wrapper">
    
    <div id="disqus_thread" class="article-comments"></div>
    <script>
      (function() {
          var d = document, s = d.createElement('script');
          s.src = '//shortivorytower.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
  </div>
</section> <!-- End Comment Area -->

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>
  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
</html>
